{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "191d93ce-8270-47d8-803f-9a7c6e0cb311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae9761-b23d-444b-9c95-3b83a159aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "file_path = \"The-magic-of-thinking-big.pdf\"\n",
    "reader = PdfReader(file_path)\n",
    "\n",
    "output_file_path = \"Book.txt\"\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "\n",
    "    for page_number, page in enumerate(reader.pages, start=1):\n",
    "        text = page.extract_text()\n",
    "        output_file.write(f\"--- Page {page_number} ---\\n\")\n",
    "        output_file.write(text)\n",
    "        output_file.write(\"\\n\\n\")\n",
    "print(f\"Text extracted and saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d667478-a205-4b2f-b5c1-0e24c45f8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Book.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ce1e2c-e40f-4254-b6c4-1fc6f0c36c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050f7ef-9ca9-48b5-b845-fea6b5a17f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b9d648-7afc-4b6c-ad09-b3b95f7797f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list) + 1):  \n",
    "        n_gram_sequence = token_list[:i]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1aa214-184f-4e18-a910-661feefef1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"input_sequences:\", input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba680548-601b-4747-a152-4cc9967f7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41733656",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6a9b4-91eb-47dd-afd6-c06f4beb17f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "544cd4d7-4825-41c9-9d2b-30783b561268",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8087e513-4fd9-4276-b1fc-27d233f4ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1dd70b-81ba-4d5f-9d05-4086a2ad3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebf456",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d68a202-3c20-4ef6-a8d9-2273f7e6fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "753dda81-2da2-43b1-8d1c-ffff8311a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history['accuracy']\n",
    "loss = history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7250ad4f-12ab-4201-9db0-adb9e1381015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed83f2a-299a-42d5-a23d-3e1724a59d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(100), accuracy, label='Training Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training Accuracy')\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(100), loss, label='Training Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce9f82-7007-4e1b-bc6a-5cba14bafebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"how to think\"\n",
    "next_words = 3\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190acac-0b99-46a8-90cb-7a9a689c7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 1\n",
    "model.save(f\"Models/{model_version}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "add81d72-5e17-4156-8d3c-52a66fbd9042",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 1\n",
    "model.save(f\"Models/{model_version}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3680c-7b0c-4d49-9623-0951e7a57fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 1\n",
    "model.export(f\"Models/{model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00d6d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
